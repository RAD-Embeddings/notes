\date{2025-08-14T04:38:46Z}
\taxon{Definition}
\title{Task-Conditioned Multi-Agent Reinforcement Learning (TC-MARL)}

\import{VBIH}  % MDP.

\def\role{\psi}
\def\task{\varphi}
\let\Roles{\Psi}
\let\Distr{\Delta}
\let\path{\xi}

\p{
  Let #{\agent_1, \ldots, \agent_n} be a collection of #{n}
  agents operating in a shared [MDP](VBIH).
  % TODO link to task definition.
  Let #{\role_i \in \Roles} denote a binary task assigned to agent #{i}.
}
\p{
  The goal of TC-MARL is to synthesize a policy,
  ##{
     \policy : \States \times \Roles \times \Roles^{n-1} \to \Distr{\Actions}
  }
  that maximizes the probability that all agent tasks are satisified, 
  i.e.,
  ##{\max_\pi \Pr_\path\left ( \bigwedge_i \role_i(\path) \mid \pi\right)}
  where #{\path \sim \policy} is a joint state path sampled 
  from the distrubtion induced by each
  agent #{i} using the policy:
  ##{
     \policy_i(\bullet \mid s) \triangleq \policy(\bullet \mid s, \role, \role_{-1}) \in \Distr{\Actions}.
  }
}
